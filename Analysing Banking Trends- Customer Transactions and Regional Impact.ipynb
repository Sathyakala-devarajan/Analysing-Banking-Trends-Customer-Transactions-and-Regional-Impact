{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05d48539-b2d2-4014-b12d-395271b55b6a",
   "metadata": {},
   "source": [
    "# Module - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f8cf18d-2211-4546-a048-c89906ca4ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Function to read the CSV file into a DataFrame\n",
    "def read_csv():\n",
    "    # read the user_nodes.csv file using pandas library and return it\n",
    "    df = pd.read_csv('user_nodes.csv')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6470db7-ca97-4652-a998-59c8fabf7674",
   "metadata": {},
   "source": [
    "# Task – 1: Check the null values\n",
    "    The function check_null_values() reads a DataFrame from read_csv  function which returns data in the user_nodes csv file and the it needs to  calculates the sum of null (missing) values for each column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6fe8b24-3b7a-4839-8234-7dcfa1167774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_null_values():\n",
    "    # do not edit the predefined function name\n",
    "    df = read_csv()\n",
    "    # Check for null values using the isnull() method and sum them for each column\n",
    "    null_values = None\n",
    "    null_values = df.isnull().sum()\n",
    "    return null_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a56c63-d2eb-4e30-a151-1cbb2579b519",
   "metadata": {},
   "source": [
    "# Task – 2: Find the Duplicate Values\n",
    "    Here, We need to check for duplicate values in a dataset for each column. \n",
    "Finally, the counts of duplicated values are returned as a integer. This information can be useful in identifying duplicate data and deciding on appropriate strategies to deal with them, such as imputation or deletion\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "157e352a-be74-4ca1-ae62-0eee70e96423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for duplicate rows in the DataFrame\n",
    "def check_duplicates():\n",
    "    # do not edit the predefined function name\n",
    "    df = read_csv()\n",
    "    # Calculate the number of duplicate rows using the duplicated() method and sum them\n",
    "    duplicates = None\n",
    "    duplicates = df.duplicated().sum()\n",
    "    return duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a559b4-0f71-4576-8797-b7c148a9fdf7",
   "metadata": {},
   "source": [
    "# Task -3: Remove the Duplicate Values\n",
    "    The function drop_duplicates() reads a DataFrame from a CSV file, removes any duplicate rows in the DataFrame, and returns the cleaned DataFrame without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2160e995-adda-4519-8ac6-043333bf029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop duplicate rows from the DataFrame\n",
    "def drop_duplicates():\n",
    "    # do not edit the predefined function name\n",
    "    df = read_csv()\n",
    "    # Drop duplicate rows using the drop_duplicates() method with inplace=True\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99968dee-5070-4987-b11f-24baa4bedf96",
   "metadata": {},
   "source": [
    "# Task – 4: Data Cleaning\n",
    "    The provided function, data_cleaning(), aims to enhance the quality and structure of a given DataFrame. It starts by removing duplicate rows to ensure data consistency. Subsequently, the function eliminates the columns \"has_loan\" and \"is_act\" from the DataFrame, streamlining it for analysis. To improve clarity, column names are adjusted: 'id_' becomes 'consumer_id', 'area_id_' is transformed to 'region_id', 'node_id_' is changed to 'node_id', 'act_date' is renamed to 'start_date', and 'deact_date' is converted to 'end_date'. The final step involves exporting the cleaned DataFrame to a CSV file named 'user_nodes_cleaned.csv', without including the index column. Upon completion, the function returns the cleaned DataFrame, now prepared for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8084384-74db-4eb0-acf2-e3599c7259cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning():\n",
    "    df = drop_duplicates()\n",
    "    # Step 3: Drop specified columns from the DataFrame(\"has_loan\", \"is_act\")\n",
    "    df.drop(columns=[\"has_loan\", \"is_act\"], inplace=True)\n",
    "\n",
    "    #Rename columns names id_,area_id_,node_id_,act_date',deact_date to  consumer_id,region_id,node_id,start_date,end_date\n",
    "    df.rename(columns={\"id_\": \"consumer_id\", \"area_id_\": \"region_id\", \"node_id_\": \"node_id\",\n",
    "                       \"act_date\": \"start_date\", \"deact_date\": \"end_date\"}, inplace=True)\n",
    "\n",
    "    df.to_csv('user_nodes_cleaned.csv', index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0d3cf-4424-464e-aea4-966313100ea5",
   "metadata": {},
   "source": [
    "# Task – 5: Check the null values\n",
    "    The function check_null_values() reads a DataFrame from read_csv  function which returns data in the user_transactions csv file and the it needs to  calculates the sum of null (missing) values for each column in the DataFrame.\r",
    "    \n",
    "It then returns a Series containing the count of null values for each column in the DataFrame. This provides insights into the presence and extent of missing data in the dataset after duplicates have been dropped.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05d6997e-90df-4dd8-9f3b-c7fbf012db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for null (missing) values in the DataFrame\n",
    "def check_null_values():\n",
    "    # do not edit the predefined function name\n",
    "    df = read_csv()\n",
    "    # Check for null values using the isnull() method and sum them for each column\n",
    "    null_values = None\n",
    "    null_values = df.isnull().sum()\n",
    "    return null_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e683e8-8eba-47b8-b6dc-3cea698a7dce",
   "metadata": {},
   "source": [
    "# Task – 6: Find the Duplicate Values\n",
    "    Here, We need to check for duplicate values in a dataset  for each column. \n",
    "Finally, the counts of duplicated values are returned as a integer. This information can be useful in identifying duplicate data and deciding on appropriate strategies to deal with them, such as imputation or deletion.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57f7e34d-abdb-45c7-ab2b-70ddbb905c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for duplicate rows in the DataFrame\n",
    "def check_duplicates():\n",
    "    # do not edit the predefined function name\n",
    "    df = read_csv()\n",
    "    # Calculate the number of duplicate rows using the duplicated() method and sum them\n",
    "    duplicates = None\n",
    "    duplicates = df.duplicated().sum()\n",
    "    return duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac0983-96bb-4aa3-971c-1947d3ded9a7",
   "metadata": {},
   "source": [
    "# Task – 7: Remove the Duplicate Values\n",
    "    The function drop_duplicates() reads a DataFrame from a CSV file, removes any duplicate rows in the DataFrame, and returns the cleaned DataFrame without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06556ad9-be08-45a1-b8e8-e073992afc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to drop duplicate rows from the DataFrame\n",
    "def drop_duplicates():\n",
    "    # do not edit the predefined function name\n",
    "    df = read_csv()\n",
    "    # Drop duplicate rows using the drop_duplicates() method with inplace=True\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e188a8ef-dbf3-4a5b-be1c-0ccfeb831bd4",
   "metadata": {},
   "source": [
    "# Task – 8: Data Cleaning\n",
    "    The function data_cleaning() is designed to enhance a DataFrame's quality by removing specific columns and renaming others, preparing the data for analysis. Initially, duplicate rows are removed and any rows with null values are dropped. Then, the columns \"has_credit_card\" and \"account_type\" are selected for removal to streamline the dataset for analysis. The DataFrame's columns are updated by renaming 'id_' to 'consumer_id', 't_date' to 'transaction_date', 't_type' to 'transaction_type', and 't_amt' to 'transaction_amount' for improved clarity and interpretation. Once these steps are completed, the cleaned DataFrame is saved as 'user_transaction_cleaned.csv' without the index column and returned for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc2e169c-bcdb-4c53-92c6-138f12da4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning():\n",
    "    \"\"\"\n",
    "    Data Cleaning Function:\n",
    "    Cleans the DataFrame by dropping specified columns and renaming others.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The cleaned DataFrame after dropping and renaming columns.\n",
    "    \"\"\"\n",
    "    # Step 1: Get the DataFrame with duplicate rows removed and rows with null values dropped\n",
    "    df = drop_duplicates()\n",
    "\n",
    "    # Step 2: Columns to remove from the DataFrame\n",
    "    #columns needs to be removed \"has_credit_card\" and  \"account_type\"\n",
    "    columns_to_remove = [\"has_credit_card\", \"account_type\"]\n",
    "    # Drop specified columns from the DataFrame\n",
    "    df.drop(columns=columns_to_remove, inplace=True)\n",
    "    #Rename columns id_,t_date,t_type,t_amt to consumer_id,transaction_date,transaction_type,transaction_amount\n",
    "    df.rename(columns={\"id_\": \"consumer_id\", \"t_date\": \"transaction_date\", \n",
    "                       \"t_type\": \"transaction_type\", \"t_amt\": \"transaction_amount\"}, inplace=True)\n",
    "\n",
    "    # Step 5: Rename columns using the new column names\n",
    "    df.to_csv('user_transactions_cleaned.csv', index=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a02f1-add0-47cd-bad4-b90bc8f93eb1",
   "metadata": {},
   "source": [
    "# Task – 9: Generate tables using the cleaned dataset\n",
    "    Download the cleaned dataset by clicking on the file name from File explorer.\r",
    "    \n",
    "You must download the world_regions csv file as well as the user nodes and user transactions cleaned csv files in order to proceed. To continue with this project, you must add all three CSV files to the database.    \r\n",
    "Utilize the MySQL database information provided in \"\"Database Details\"\" to manually create the following tables for the cleaned dataset\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6fdee8-5d2a-4936-b5f4-2705e8b62528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
